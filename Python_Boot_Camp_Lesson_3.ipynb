{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharmaar342/sharmaar342/blob/main/Python_Boot_Camp_Lesson_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzADHhh5kAPv"
      },
      "source": [
        "# Python Boot Camp Lesson 3\n",
        "\n",
        "**Author:** Nicholas Colella<br>\n",
        "**Date created:** 2021/08/15<br>\n",
        "**Last modified:** 2021/01/24<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Course Information\n",
        "\n",
        "You are encouraged to watch the corresponding video, available on Canvas, as you work through this notebook. \n",
        "\n",
        "Additionally, we strongly encourage you to test your understanding of the material as you go! The Canvas quiz can be completed while you watch the video and work on the notebook (and can be taken multiple times)."
      ],
      "metadata": {
        "id": "J56fPwiMX21n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyl6YYk60ZXM"
      },
      "source": [
        "# Pandas DataFrames\n",
        "\n",
        "Recall from the last lesson we created a NumPy array then saved it to a .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mGcxq0e58X0"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-gokoKH6Ylm"
      },
      "source": [
        "my_matrix_1 = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "print(my_matrix_1)\n",
        "\n",
        "np.savetxt('./gdrive/MyDrive/my_array.csv', my_matrix_1, delimiter=',')\n",
        "loaded_array = np.loadtxt('./gdrive/MyDrive/my_array.csv', delimiter=',')\n",
        "loaded_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG9nNLBM6ODj"
      },
      "source": [
        "Oftentimes we have labeled data, and we want to present our data more cleanly in a spreadsheet-type manner. This is similar to working with Excel. For this, we use pandas in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJU-gLjSz0gL"
      },
      "source": [
        "pd.DataFrame(loaded_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXtOXRvr01jD"
      },
      "source": [
        "We can also load csv files directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kVRkDDm0xg-"
      },
      "source": [
        "pd.read_csv('./gdrive/MyDrive/my_array.csv') # this assumes our first row is the header!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMrwsWeb06WC"
      },
      "source": [
        "my_df = pd.read_csv('./gdrive/MyDrive/my_array.csv', header=None) # generates a header for us\n",
        "my_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eEAhSY81x6r"
      },
      "source": [
        "We can rename our rows (indices) and columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6Vo-bCE1C_r"
      },
      "source": [
        "my_df.columns = ['first col', 'second col', 'third col'] # we can also assign these during import with read_csv, see https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
        "my_df.index = ['first row', 'second row'] # Can be pulled with read_csv if present in file, see https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
        "my_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3HEC7w-140n"
      },
      "source": [
        "We can now access data either with the given **name** with `.loc` or by the numeric **index** with `.iloc`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejLjD49733gY"
      },
      "source": [
        "Specific point:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVwfjK7p1S10"
      },
      "source": [
        "print(my_df.loc['first row', 'second col'])\n",
        "print(my_df.iloc[0, 1])\n",
        "print(my_df.iloc[0][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcQwUFR537YI"
      },
      "source": [
        "A given column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbA1Lu0A4NGt"
      },
      "source": [
        "print(my_df['second col'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkUmeL8G4Pei"
      },
      "source": [
        "print(my_df.loc[:, 'second col'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHrxngzm3EHL"
      },
      "source": [
        "print(my_df.iloc[:, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSvgnnKL3egh"
      },
      "source": [
        "# print(my_df.iloc[:][1]) # Errors, can't use this"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xea8r6l4aSR"
      },
      "source": [
        "A given row:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etpVgm1I3cUk"
      },
      "source": [
        "print(my_df.loc['first row'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARE_8Z2-4xg4"
      },
      "source": [
        "print(my_df.loc['first row', :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9I-dlqp1c-B"
      },
      "source": [
        "print(my_df.iloc[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBi013-t4hsa"
      },
      "source": [
        "print(my_df.iloc[0, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhsMgvhlzyqQ"
      },
      "source": [
        "# Data\n",
        "\n",
        "While you can upload data to Colab directly or via Google Drive, you can also pull data from online sources. We can use `wget` to download a zip file from a website to our Colab runtime. The `!` before `wget` signifies that this is a bash command (usually executed in terminal), *not* a Python command. Similarly, we can use the bash command `unzip` to unzip the file, again using `!` to signify that it should be run in bash, not Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXs0BtZU_-zz"
      },
      "source": [
        "!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
        "!unzip jena_climate_2009_2016.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRlAOVcy0hWr"
      },
      "source": [
        "Now that we have unzipped the data, we can load it into a Pandas dataframe called `weather_df` with the function `pd.read_csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLyumOAD8Wlv"
      },
      "source": [
        "weather_df = pd.read_csv('jena_climate_2009_2016.csv')\n",
        "weather_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqfBMERS0uhP"
      },
      "source": [
        "We can also open online files directly with the urllib request module. `Request` accesses a website like a web browser would, and we can specify its behavior with `add_header`. We can then use `urlopen` to store the data from that website in a variable. If we access a .csv file this way, we can then format it into a pandas dataframe. In this method, the .csv file is never stored locally or on the Colab runtime, it is only used to generate the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q65U607US_7y"
      },
      "source": [
        "from urllib.request import Request, urlopen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyTObSRnLS_7"
      },
      "source": [
        "req = Request('http://files.direxionfunds.com/DirexionWebsiteFiles/holdings_moon.csv')\n",
        "req.add_header('User-Agent', 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:77.0) Gecko/20100101 Firefox/77.0')\n",
        "content = urlopen(req)\n",
        "df1 = pd.read_csv(content, skiprows=5)\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPuSELYJ1oWS"
      },
      "source": [
        "We can also attempt to access files directly using pandas, for instance by using `pd.read_excel` to access an online .xlsx file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAO4EGoOMhjY"
      },
      "source": [
        "df2 = pd.read_excel('https://www.ssga.com/us/en/institutional/etfs/library-content/products/fund-data/etfs/us/holdings-daily-us-en-cnrg.xlsx', skiprows=4)\n",
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fii-qoFL2kL0"
      },
      "source": [
        "## Examining and cleaning data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM07LzZu13D0"
      },
      "source": [
        "Now let's take a look at the data we've obtained. We will work with the `weather_df` dataframe. Often times, we just want to inspect some of our data to make sure that it was loaded as we want. One way to get a sample of the data is to use the `.head()` function to print out the first few rows of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em1Gl2hizuyf"
      },
      "source": [
        "weather_df.head() # first 5 rows by default"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crW5C-UwO02O"
      },
      "source": [
        "weather_df.head(10) # first 10 rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YSYzj4p2NvY"
      },
      "source": [
        "Similarly, we can use `.tail()` to look at the last few rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAwEmH1D2Sdm"
      },
      "source": [
        "weather_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjMive-O2Uyf"
      },
      "source": [
        "Finally, we can also summarize the data with `.describe()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jUFZ78n2Zhd"
      },
      "source": [
        "weather_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID27LeJc2bL3"
      },
      "source": [
        "We may wish to change the formatting for some columns. `datetime` is a standardized way of representing date and time. Let's take our `'Date Time'` column and standardize its formatting. To do so, we use the `pd.to_datetime` function on the desired column, then assign the result to the column. Take a moment to ensure you understand this methodology of assigning a column based on itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BKoLJuFzw7o"
      },
      "source": [
        "weather_df['Date Time'] = pd.to_datetime(weather_df['Date Time'], format='%d.%m.%Y %H:%M:%S')\n",
        "weather_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JpmllXxL_Ca"
      },
      "source": [
        "One of the first things we want to do when importing data is ensure that we are not missing any values. We can check each value to see if it is `nan` with `.isnull()`, which will return a `True`/`False` for every value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWeol1zZMZ53"
      },
      "source": [
        "weather_df.isnull()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLU_fPGtMchz"
      },
      "source": [
        "Instead of going through one by one to see if there are any `True`s, we can use `.any()` to check for us!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai3Hq16kLBsO"
      },
      "source": [
        "weather_df.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXeCUuFGMxz4"
      },
      "source": [
        "This dataset was pre-cleaned for us, so we see that it is not missing any values! Let's go ahead a delete a value so we can look at ways of rectifying it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R0zwCQzL4J0"
      },
      "source": [
        "weather_df.iloc[1, 3] = np.nan\n",
        "weather_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86zaA8W2Mo-R"
      },
      "source": [
        "weather_df.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMHq7a2wNhUa"
      },
      "source": [
        "We must now decide what we want to do with rows/columns that are missing data. We can remove the row or column with the `.dropna()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY1-Lc3BNShe"
      },
      "source": [
        "weather_df_dropped_row = weather_df.dropna(axis=0)\n",
        "weather_df_dropped_row.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPBO2cTgODX_"
      },
      "source": [
        "weather_df_dropped_column = weather_df.dropna(axis=1)\n",
        "weather_df_dropped_column.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNCdzJOSOLzQ"
      },
      "source": [
        "Oftentimes, however, we want to keep a data entry even if is incomplete (i.e. there are `NaN`s present). To do so, we can use `.ffill()` or `.bfill()` to fill in the missing data point with the previous or next data point, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX0T9oEMOJ-K"
      },
      "source": [
        "weather_df_forward_fill = weather_df.ffill()\n",
        "weather_df_forward_fill.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9rHgveTOkyu"
      },
      "source": [
        "weather_df_back_fill = weather_df.bfill()\n",
        "weather_df_back_fill.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF_Q6m0uPgaq"
      },
      "source": [
        "Alternatively, we can fill in the point using interpolation, looking and neighboring points to guess what the missing point is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9metaDrkOqs0"
      },
      "source": [
        "weather_df_interp = weather_df.interpolate()\n",
        "weather_df_interp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4JINvnURs_l"
      },
      "source": [
        "Finally, we can also choose a new index for our dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-ghRXrxRshf"
      },
      "source": [
        "weather_df = weather_df_interp.set_index('Date Time')\n",
        "weather_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAg-EFxvQt3T"
      },
      "source": [
        "## Slicing and combining dataframes\n",
        "\n",
        "Sometimes we are interested in only a portion of our dataset, and other times we have multiple datasets that we would like to combine into one. This is where slicing and combining come into play, respectively.\n",
        "\n",
        "Let's look at the first day of data and only look at pressure (`p (mbar)` and temperature `T (degC)`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3awq6pe1SoLC"
      },
      "source": [
        "import datetime as dt # we want to create a datetime object outside of the dataframe\n",
        "end_time = dt.datetime(2009, 1, 1, 1, 0, 0) # 2009-01-01 01:00:00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLe1mkNlQK6I"
      },
      "source": [
        "first_slice = weather_df.loc[weather_df.index <= end_time, ['p (mbar)', 'T (degC)']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDpysYJzSzBh"
      },
      "source": [
        "first_slice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbFueUhoTtqa"
      },
      "source": [
        "For a slice of the second day, we need a start and end time. The easiest way to use multiple inequalities is with `np.logical_and()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuGLgXgmSzum"
      },
      "source": [
        "end_time_2 = dt.datetime(2009, 1, 1, 2, 0, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYaoBGrwUN9T"
      },
      "source": [
        "second_slice = weather_df.loc[np.logical_and(weather_df.index > end_time, weather_df.index <= end_time_2), ['p (mbar)', 'T (degC)']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk3RB50eUUaD"
      },
      "source": [
        "second_slice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppPMjCKOVgHu"
      },
      "source": [
        "Now let's make things a bit more interesting and make a slice that overlaps with the first day, but also contains data from the second day. Let us also include `Tpot (K)` in this slice, but not include `p (mbar)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2uWP4nFVo2p"
      },
      "source": [
        "overlap_start_time = dt.datetime(2009, 1, 1, 0, 30, 0)\n",
        "overlap_end_time = dt.datetime(2009, 1, 1, 1, 30, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALSUX-ZBUZSK"
      },
      "source": [
        "overlap_slice = weather_df.loc[np.logical_and(weather_df.index > overlap_start_time, weather_df.index <= overlap_end_time), ['T (degC)','Tpot (K)']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXc73OQ5V4t4"
      },
      "source": [
        "overlap_slice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDkeOmuxV7RH"
      },
      "source": [
        "### `merge()`\n",
        "\n",
        "`merge()` allows us to combine dataframes with a choice `how`. We have a number of options:\n",
        "\n",
        "*   `'inner'` - only include rows where there is data for every column\n",
        "*   `'outer'` - include any row for which we have data\n",
        "*   `'left'` - include any row for which there is data in the first ('left') dataframe\n",
        "*   `'right'` - include any row for which there is data in the second ('right') dataframe\n",
        "\n",
        "Note that for any columns that are present in both dataframes, they are kept separate when using `.merge()` **if** we pass `left_index=True, right_index=True`. See further below for how to merge columns where possible.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWnvFmYQV5ue"
      },
      "source": [
        "df_merge_inner = pd.merge(first_slice, overlap_slice, left_index=True, right_index=True, how='inner')\n",
        "df_merge_inner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7VvHhO4gVqz"
      },
      "source": [
        "df_merge_outer = pd.merge(first_slice, overlap_slice, left_index=True, right_index=True, how='outer')\n",
        "df_merge_outer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj0_SbeggX0J"
      },
      "source": [
        "df_merge_left = pd.merge(first_slice, overlap_slice, left_index=True, right_index=True, how='left')\n",
        "df_merge_left"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY6TT0oVgZI2"
      },
      "source": [
        "df_merge_right = pd.merge(first_slice, overlap_slice, left_index=True, right_index=True, how='right')\n",
        "df_merge_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v837LALLOWfR"
      },
      "source": [
        "Note, we have been including the indices (`left_index=True, right_index=True`) which keeps the columns distinct. If we want to let Pandas merge columns where possible, we can use the default `left_indext=False, right_index=False`. First, we will reset the indices so that 'Date Time' data is retained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXvPzTTOOkw3"
      },
      "source": [
        "first_slice_no_index = first_slice.reset_index()\n",
        "first_slice_no_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5kyho9xOry6"
      },
      "source": [
        "overlap_slice_no_index = overlap_slice.reset_index()\n",
        "overlap_slice_no_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjat3vOTOzB6"
      },
      "source": [
        "df_merge_inner_no_index = pd.merge(first_slice_no_index, overlap_slice_no_index, left_index=False, right_index=False, how='inner')\n",
        "df_merge_inner_no_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4Va0xsiO7Lr"
      },
      "source": [
        "df_merge_outer_no_index = pd.merge(first_slice_no_index, overlap_slice_no_index, left_index=False, right_index=False, how='outer')\n",
        "df_merge_outer_no_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBJy6mKHUvuh"
      },
      "source": [
        "### .append()\n",
        "\n",
        "`.append()` simply sticks one dataframe onto the end of another. We can include the index or not, but it doesn't make a difference of how the dataframes are stuck together (both end up with 12 rows in this case)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT6I9pg3fpEg"
      },
      "source": [
        "df_append_with_index = first_slice.append(overlap_slice, ignore_index=False)\n",
        "df_append_with_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxhqzqpKg19n"
      },
      "source": [
        "df_append_no_index = first_slice.append(overlap_slice, ignore_index=True)\n",
        "df_append_no_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVnb1SBQVtUO"
      },
      "source": [
        "### concat()\n",
        "\n",
        "Again we have the options of `'outer'` which will include any columns which contain data, and `'inner'` which will only include columns which have data in all dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TKWea5AhmSB"
      },
      "source": [
        "df_concat_outer = pd.concat([first_slice, overlap_slice], join='outer')\n",
        "df_concat_outer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHNIxbLPh3p_"
      },
      "source": [
        "df_concat_inner = pd.concat([first_slice, overlap_slice], join='inner')\n",
        "df_concat_inner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQfltxGRWm9Z"
      },
      "source": [
        "### `.combine_first`\n",
        "\n",
        "`.combine_first` is used when we have a one dataframe that has priority over the other. This way, if both dataframes have the data for a given element (row-column pair), the data from the first dataframe will be retained.\n",
        "\n",
        "This is *generally* my preferred way of combining dataframes, as it sets a clear priority if there are conflicting values, while also resulting in the most complete dataset possible. That said, the method of choice will depend on the problem at hand!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1n7J-FIi2G7"
      },
      "source": [
        "df_combine_first = first_slice.combine_first(overlap_slice)\n",
        "df_combine_first"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5jeo48LXbt2"
      },
      "source": [
        "A more illustrative example of `.combine_first`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP4WjMLQXa9l"
      },
      "source": [
        "s = pd.DataFrame([1, 2, np.nan])\n",
        "t = pd.DataFrame([np.nan, 1, 3, 4])\n",
        "print(s)\n",
        "print(' ')\n",
        "print(t)\n",
        "print(' ')\n",
        "print(s.combine_first(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9tX2nggYG2f"
      },
      "source": [
        "### `.update()`\n",
        "\n",
        "`.update()` behaves a bit differently, as it always modifies the dataframe in-place. It will overwrite any data in the first dataframe with data from the second dataframe, but will retain the same rows/columns as the first dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZcZ3oGSiP8x"
      },
      "source": [
        "first_slice_updated = first_slice.copy()\n",
        "first_slice_updated.update(overlap_slice)\n",
        "first_slice_updated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP5zb9XXYd1d"
      },
      "source": [
        "print(s)\n",
        "print(t)\n",
        "s_updated = s.copy()\n",
        "s_updated.update(t)\n",
        "print(s_updated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKlMwKXCCOVj"
      },
      "source": [
        "# Plotting\n",
        "\n",
        "It is often useful to have a visual representation of your data, e.g. via plotting. The most commonly used library for plotting with Python is matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhU_LlnUirEf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.facecolor'] = 'white' # sets the default background color to white"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hha5BdTEC3Nd"
      },
      "source": [
        "The most simple way to plot is `plt.plot(x_vals, y_vals)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMQFgRc3EOWg"
      },
      "source": [
        "plt.plot([0, 1, 2], [0, 5, 4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIl7o9APEVsK"
      },
      "source": [
        "Plotting a dataframe column will select the index for the x values if not specified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOSh_GSQClNZ"
      },
      "source": [
        "plt.plot(first_slice['T (degC)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn2SBk3oDgu6"
      },
      "source": [
        "However, a good plot should label its axes and have a title and legend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EToUTJ8bDm6A"
      },
      "source": [
        "plt.plot(first_slice['T (degC)'], label='T (degC)', color='r')\n",
        "plt.xlabel('Date Time')\n",
        "plt.ylabel('T (degC)')\n",
        "plt.title('Temp vs Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCBuoeuLEDiH"
      },
      "source": [
        "Much better. Now, what if we want to have multiple plots as part of the same figure?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiWbNBo7C2VT"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10,7))\n",
        "\n",
        "ax.plot(first_slice['T (degC)'], color='r', label='T (degC)')\n",
        "ax.tick_params(axis='y', colors='r')\n",
        "ax.set_xlabel('Date time', size=20) \n",
        "ax.set_ylabel('Temp', size=14, color='r')\n",
        "ax.set_title('Temp and Pressure, First Hour', size=24)\n",
        "\n",
        "ax.axhline(-8.3, color=\"gray\") # we can add an arbitrary horizontal line\n",
        "\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(first_slice['p (mbar)'], color='b', label='p (mbar)')\n",
        "ax2.spines[\"right\"].set_color('b')\n",
        "ax2.tick_params(axis='y', colors='b')\n",
        "ax2.set_ylabel('Pressure', color='b', size=14)\n",
        "\n",
        "ax2.text(first_slice.index[-1], first_slice['p (mbar)'][-1], first_slice['p (mbar)'][-1], fontsize=15, color='b', horizontalalignment='center')\n",
        "fig.patch.set_facecolor('w')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hkPfpqf7D_g"
      },
      "source": [
        "That's all for Lesson 3! In our next and final lesson, we will examine flow control and functions!\n",
        "\n",
        "In the meantime, check out Quiz 3 on Canvas!"
      ]
    }
  ]
}